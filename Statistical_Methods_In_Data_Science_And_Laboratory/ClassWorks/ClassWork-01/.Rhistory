xlab = "Return")
par(mfrow = c(1, 1))
plot(pair_data$stock, pair_data$index,
main = paste(stock_name, "vs", index_name, "returns"),
xlab = paste(stock_name, "return"),
ylab = paste(index_name, "return"))
abline(lm(pair_data$index ~ pair_data$stock), col = "red")
par(mfrow = c(1, 2))
qqnorm(pair_data$stock,
main = paste("QQ-plot of", stock_name, "returns"))
qqline(pair_data$stock)
qqnorm(pair_data$index,
main = paste("QQ-plot of", index_name, "returns"))
qqline(pair_data$index)
par(mfrow = c(1, 1))  # final reset
set.seed(123)   # for reproducibility
## 1. Choose dimension p and sample size n
p <- 2          # small dimension (as suggested)
n <- 1000       # number of observations for each vector
# Data from F_X
X <- matrix(rnorm(n * p, mean = 0, sd = 1), nrow = n, ncol = p)
# Data from F_Y (different mean)
mu_Y <- c(1, 1)   # for p = 2
Y <- matrix(NA, nrow = n, ncol = p)
for (j in 1:p) {
Y[, j] <- rnorm(n, mean = mu_Y[j], sd = 1)
}
colnames(X) <- paste0("X", 1:p)
colnames(Y) <- paste0("Y", 1:p)
## 3. Function to generate a random unit vector gamma ~ something like Unif on [0,1]^p
gen_gamma <- function(p) {
g <- runif(p, min = 0, max = 1)
g / sqrt(sum(g^2))   # normalize to have norm 1
}
## 4. Choose how many random directions gamma to try
n_gamma <- 3
## 5. For each gamma: project X and Y, compare distributions
for (k in 1:n_gamma) {
cat("=====================================\n")
cat("Projection", k, "\n")
# 5.1 Generate a random unit vector gamma
gamma <- gen_gamma(p)
cat("gamma =", gamma, "\n\n")
# 5.2 Compute projections gamma^T X_i and gamma^T Y_i
proj_X <- as.vector(X %*% gamma)
proj_Y <- as.vector(Y %*% gamma)
# 5.3 Numerical summaries of projections
cat("Summary of gamma^T X:\n")
print(summary(proj_X))
cat("sd(gamma^T X) =", sd(proj_X), "\n\n")
cat("Summary of gamma^T Y:\n")
print(summary(proj_Y))
cat("sd(gamma^T Y) =", sd(proj_Y), "\n\n")
# 5.4 Numerical distance between the two projected distributions
#     Here I use the two-sample Kolmogorov–Smirnov test
ks_res <- ks.test(proj_X, proj_Y)
cat("KS statistic:", ks_res$statistic, "\n")
cat("KS p-value :", ks_res$p.value, "\n\n")
# 5.5 Visual comparison via histograms
par(mfrow = c(1, 2))
hist(proj_X,
main = paste("Projection", k, ": gamma^T X"),
xlab = expression(gamma^T * X),
col = "lightblue", border = "white")
hist(proj_Y,
main = paste("Projection", k, ": gamma^T Y"),
xlab = expression(gamma^T * Y),
col = "lightgreen", border = "white")
par(mfrow = c(1, 1))
}
set.seed(123)   # for reproducibility
set.seed(123)   # for reproducibility
## 1. Sample size
n <- 5000
## 2. Generate X and Z ~ Unif(-1, 1), independent
X <- runif(n, min = -1, max = 1)
Z <- runif(n, min = -1, max = 1)
## 3. Build Y according to the rule in the text
##    Y = Z if sign(X) == sign(Z), else Y = -Z
same_sign <- (X >= 0 & Z >= 0) | (X < 0 & Z < 0)
Y <- ifelse(same_sign, Z, -Z)
## Quick check
head(cbind(X, Z, Y))
summary(X)
summary(Y)
## Pearson correlation (linear dependence)
cor_pearson <- cor(X, Y)
cor_pearson
## Spearman correlation (rank-based, monotone)
cor_spearman <- cor(X, Y, method = "spearman")
cor_spearman
## 5.1 Scatterplot of X vs Y
plot(X, Y,
main = "Scatterplot of X vs Y",
xlab = "X",
ylab = "Y",
pch = 19, cex = 0.5)
## 5.2 Histograms of X and Y
par(mfrow = c(1, 2))
hist(X,
main = "Histogram of X ~ Unif(-1, 1)",
xlab = "X",
col = "lightblue", border = "white")
hist(Y,
main = "Histogram of Y (constructed from X, Z)",
xlab = "Y",
col = "lightgreen", border = "white")
par(mfrow = c(1, 1))  # reset layout
if (!requireNamespace("minerva", quietly = TRUE)) {
mic_value <- NA
warning("Package 'minerva' is not installed. Run install.packages('minerva') to compute MIC.")
} else {
library(minerva)
## mine() can take two vectors X and Y
mic_res <- mine(X, Y)
## The MIC value is stored in mic_res$MIC
mic_value <- mic_res$MIC
}
cat("Pearson correlation:", cor_pearson, "\n")
cat("Spearman correlation:", cor_spearman, "\n")
cat("MIC(X, Y):", mic_value, "\n")
###############################################
###############################################
# In the report you can comment:
###############################################
# In the report you can comment:
# - The scatterplot structure (nonlinear dependence).
###############################################
# In the report you can comment:
# - The scatterplot structure (nonlinear dependence).
# - Pearson/Spearman values.
###############################################
# In the report you can comment:
# - The scatterplot structure (nonlinear dependence).
# - Pearson/Spearman values.
# - MIC (if computed) as a measure of nonlinear association.
###############################################
# In the report you can comment:
# - The scatterplot structure (nonlinear dependence).
# - Pearson/Spearman values.
# - MIC (if computed) as a measure of nonlinear association.
###############################################
# Load the data
grsp <- read.csv("CRSPday.csv")
# Pick two variables: GE and IBM returns
x <- grsp$ge
y <- grsp$ibm
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "GE returns", xlab = "GE")
hist(data$index, main = "IBM returns", xlab = "IBM")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "GE vs IBM", xlab = "GE", ylab = "IBM")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
x <- grsp$ge
y <- grsp$mobil
data <- na.omit(data.frame(stock = x, index = y))
summary(data)
cor(data$stock, data$index)
colMeans(data)
var(data)
hist(data$stock, main = "GE returns", xlab = "GE")
hist(data$index, main = "Mobil returns", xlab = "Mobil")
plot(data$stock, data$index,
main = "GE vs Mobil", xlab = "GE", ylab = "Mobil")
abline(lm(index ~ stock, data = data), col = "red")
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
x <- grsp$ge
y <- grsp$crsp
data <- na.omit(data.frame(stock = x, index = y))
summary(data)
cor(data$stock, data$index)
colMeans(data)
var(data)
hist(data$stock, main = "GE returns", xlab = "GE")
hist(data$index, main = "CRSP returns", xlab = "CRSP")
plot(data$stock, data$index,
main = "GE vs CRSP", xlab = "GE", ylab = "CRSP")
abline(lm(index ~ stock, data = data), col = "red")
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
x <- grsp$ibm
y <- grsp$mobil
data <- na.omit(data.frame(stock = x, index = y))
summary(data)
cor(data$stock, data$index)
colMeans(data)
var(data)
hist(data$stock, main = "IBM returns", xlab = "IBM")
hist(data$index, main = "Mobil returns", xlab = "Mobil")
plot(data$stock, data$index,
main = "IBM vs Mobil", xlab = "IBM", ylab = "Mobil")
abline(lm(index ~ stock, data = data), col = "red")
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
x <- grsp$ibm
y <- grsp$crsp
data <- na.omit(data.frame(stock = x, index = y))
summary(data)
# Load the data
grsp <- read.csv("CRSPday.csv")
# Pick two variables: GE and IBM returns
x <- grsp$ge
y <- grsp$ibm
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "GE returns", xlab = "GE")
hist(data$index, main = "IBM returns", xlab = "IBM")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "GE vs IBM", xlab = "GE", ylab = "IBM")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
# Load the data
grsp <- read.csv("CRSPday.csv")
# Pick two variables: GE and Mobil returns
x <- grsp$ge
y <- grsp$mobil
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "GE returns", xlab = "GE")
hist(data$index, main = "Mobil returns", xlab = "Mobil")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "GE vs Mobil", xlab = "GE", ylab = "Mobil")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
# Pick two variables: GE and CRSP index returns
x <- grsp$ge
y <- grsp$crsp
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "GE returns", xlab = "GE")
hist(data$index, main = "CRSP returns", xlab = "CRSP")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "GE vs CRSP", xlab = "GE", ylab = "CRSP")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
# Pick two variables: IBM and Mobil returns
x <- grsp$ibm
y <- grsp$mobil
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "IBM returns", xlab = "IBM")
hist(data$index, main = "Mobil returns", xlab = "Mobil")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "IBM vs Mobil", xlab = "IBM", ylab = "Mobil")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
# Pick two variables: IBM and CRSP index returns
x <- grsp$ibm
y <- grsp$crsp
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "IBM returns", xlab = "IBM")
hist(data$index, main = "CRSP returns", xlab = "CRSP")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "IBM vs CRSP", xlab = "IBM", ylab = "CRSP")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
# Pick two variables: Mobil and CRSP index returns
x <- grsp$mobil
y <- grsp$crsp
# Create a clean data frame with no missing values
data <- na.omit(data.frame(stock = x, index = y))
# Look at summaries
summary(data)
cor(data$stock, data$index)
# Estimate mean and covariance
colMeans(data)
var(data)
# Plot histograms
hist(data$stock, main = "Mobil returns", xlab = "Mobil")
hist(data$index, main = "CRSP returns", xlab = "CRSP")
# Scatterplot with regression line
plot(data$stock, data$index,
main = "Mobil vs CRSP", xlab = "Mobil", ylab = "CRSP")
abline(lm(index ~ stock, data = data), col = "red")
# QQ-plots to check normality
qqnorm(data$stock); qqline(data$stock)
qqnorm(data$index); qqline(data$index)
set.seed(123)   # for reproducibility
set.seed(123)   # we fix the seed so we can reproduce the result
# we choose dimension and sample size
p <- 2
n <- 1000
# we simulate X ~ N_p(0, I)
X <- cbind(
rnorm(n, mean = 0, sd = 1),
rnorm(n, mean = 0, sd = 1)
)
# we simulate Y ~ N_p(mu, I) with a different mean
mu <- c(1, 1)
Y <- cbind(
rnorm(n, mean = mu[1], sd = 1),
rnorm(n, mean = mu[2], sd = 1)
)
colnames(X) <- c("X1", "X2")
colnames(Y) <- c("Y1", "Y2")
# we generate one random unit vector gamma
g <- runif(p, 0, 1)
gamma <- g / sqrt(sum(g^2))
gamma
# we project X and Y on gamma: gamma^T X and gamma^T Y
proj_X <- as.numeric(X %*% gamma)
proj_Y <- as.numeric(Y %*% gamma)
# we look at basic summaries
summary(proj_X)
summary(proj_Y)
# we compare the two projected distributions with a KS test
ks.test(proj_X, proj_Y)
# we compare them visually with histograms
par(mfrow = c(1, 2))
hist(proj_X, main = "gamma^T X", xlab = "projection of X")
hist(proj_Y, main = "gamma^T Y", xlab = "projection of Y")
par(mfrow = c(1, 1))
set.seed(123)
# 1) We choose dimension p and sample size n
p <- 2
n <- 1000
# 2) We simulate two different bivariate distributions:
#    X ~ N((0,0), I), Y ~ N((1,1), I)
X1 <- rnorm(n, mean = 0, sd = 1)
X2 <- rnorm(n, mean = 0, sd = 1)
X  <- cbind(X1, X2)
Y1 <- rnorm(n, mean = 1, sd = 1)
Y2 <- rnorm(n, mean = 1, sd = 1)
Y  <- cbind(Y1, Y2)
# 3) We draw one random direction gamma and make it unit length
gamma <- runif(p)
gamma <- gamma / sqrt(sum(gamma^2))
gamma
# 4) We project X and Y on gamma: gamma^T X, gamma^T Y
proj_X <- as.vector(X %*% gamma)
proj_Y <- as.vector(Y %*% gamma)
# 5) We compare the two 1D distributions
summary(proj_X)
summary(proj_Y)
# Numerical check: KS test
ks.test(proj_X, proj_Y)
# Visual check: histograms side by side
par(mfrow = c(1, 2))
hist(proj_X, main = "gamma^T X", xlab = "projection of X")
hist(proj_Y, main = "gamma^T Y", xlab = "projection of Y")
par(mfrow = c(1, 1))
set.seed(123)   # for reproducibility
## 1. Sample size
n <- 5000
## 2. Generate X and Z ~ Unif(-1, 1), independent
X <- runif(n, min = -1, max = 1)
set.seed(123)
# 1) We pick the dimension (2D points) and how many points we simulate
p <- 2
n <- 1000
# 2) We create two different 2D clouds of points:
#    X is centered around (0,0), Y is centered around (1,1)
X1 <- rnorm(n, mean = 0, sd = 1)
X2 <- rnorm(n, mean = 0, sd = 1)
X  <- cbind(X1, X2)
Y1 <- rnorm(n, mean = 1, sd = 1)
Y2 <- rnorm(n, mean = 1, sd = 1)
Y  <- cbind(Y1, Y2)
# 3) We choose a random direction in 2D and rescale it to have length 1
gamma <- runif(p)
gamma <- gamma / sqrt(sum(gamma^2))
gamma
# 4) We project all points of X and Y on this direction
proj_X <- as.vector(X %*% gamma)
proj_Y <- as.vector(Y %*% gamma)
# 5) We look at basic summaries of the two 1D samples
summary(proj_X)
summary(proj_Y)
# Numerical check: test if the two 1D samples look like they come
# from the same distribution
ks.test(proj_X, proj_Y)
# Visual check: compare the two histograms side by side
par(mfrow = c(1, 2))
hist(proj_X, main = "Projection of X", xlab = "values")
hist(proj_Y, main = "Projection of Y", xlab = "values")
par(mfrow = c(1, 1))
set.seed(123)
# 1) Number of points to simulate
n <- 5000
# 2) generating X and Z as independent uniforms on [-1, 1]
X <- runif(n, min = -1, max = 1)
Z <- runif(n, min = -1, max = 1)
# We build Y:
# if X and Z have the same sign,  Y = Z
# otherwise, Y = -Z
same_sign <- (X >= 0 & Z >= 0) | (X < 0 & Z < 0)
Y <- ifelse(same_sign, Z, -Z)
head(cbind(X, Z, Y))
summary(X)
summary(Y)
# Pearson (usual linear correlation)
cor_pearson <- cor(X, Y)
# Spearman (based on ranks)
cor_spearman <- cor(X, Y, method = "spearman")
cor_pearson
cor_spearman
# plot X vs Y to see the shape of the relationship
plot(X, Y,
main = "Scatterplot of X vs Y",
xlab = "X",
ylab = "Y",
pch = 19, cex = 0.4)
# histograms of X and Y
par(mfrow = c(1, 2))
hist(X, main = "Histogram of X", xlab = "X")
hist(Y, main = "Histogram of Y", xlab = "Y")
par(mfrow = c(1, 1))
# computing MIC(X, Y) if the minerva package is available, we use mine() from that package
if (!requireNamespace("minerva", quietly = TRUE)) {
mic_value <- NA
warning("Package 'minerva' is not installed. Run install.packages('minerva') to get MIC.")
} else {
library(minerva)
mic_res  <- mine(X, Y)
mic_value <- mic_res$MIC
}
# printing all the dependence measures together
cat("Pearson correlation:", cor_pearson, "\n")
cat("Spearman correlation:", cor_spearman, "\n")
cat("MIC(X, Y):", mic_value, "\n")
# From the plots and the numbers we see:
# From the plots and the numbers we see:
# - X and Y both look roughly uniform on [-1, 1] on their own.
# From the plots and the numbers we see:
# - X and Y both look roughly uniform on [-1, 1] on their own.
# - In the scatterplot, points only appear in the top-right and
# From the plots and the numbers we see:
# - X and Y both look roughly uniform on [-1, 1] on their own.
# - In the scatterplot, points only appear in the top-right and
#   bottom-left quadrants → X and Y almost always have the same sign.
# From the plots and the numbers we see:
# - X and Y both look roughly uniform on [-1, 1] on their own.
# - In the scatterplot, points only appear in the top-right and
#   bottom-left quadrants → X and Y almost always have the same sign.
# - Pearson and Spearman are clearly positive but < 1.
# From the plots and the numbers we see:
# - X and Y both look roughly uniform on [-1, 1] on their own.
# - In the scatterplot, points only appear in the top-right and
#   bottom-left quadrants → X and Y almost always have the same sign.
# - Pearson and Spearman are clearly positive but < 1.
# - MIC is (almost) 1, which tells us that the dependence between X and Y
# From the plots and the numbers we see:
# - X and Y both look roughly uniform on [-1, 1] on their own.
# - In the scatterplot, points only appear in the top-right and
#   bottom-left quadrants → X and Y almost always have the same sign.
# - Pearson and Spearman are clearly positive but < 1.
# - MIC is (almost) 1, which tells us that the dependence between X and Y
#   is very strong and highly structured, even if it is not just a straight line.
